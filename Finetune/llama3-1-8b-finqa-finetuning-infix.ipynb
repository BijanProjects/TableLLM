{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install unsloth\n# Also get the latest nightly Unsloth!\n!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:40:05.845948Z","iopub.execute_input":"2025-01-14T17:40:05.846238Z","iopub.status.idle":"2025-01-14T17:43:14.901687Z","shell.execute_reply.started":"2025-01-14T17:40:05.846215Z","shell.execute_reply":"2025-01-14T17:43:14.900554Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nmodel_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/gemma-2-9b-bnb-4bit\",\n    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:43:14.903118Z","iopub.execute_input":"2025-01-14T17:43:14.903365Z","iopub.status.idle":"2025-01-14T17:43:52.441228Z","shell.execute_reply.started":"2025-01-14T17:43:14.903346Z","shell.execute_reply":"2025-01-14T17:43:52.440169Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc2c1b237f21434c9dd6d5786f976719"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7d6fbc84654457ba0c5093ee1582c49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f407d288ebbc4e48847b689642fe743a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5560f09eb7a14fb69a156dc88e525436"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/340 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18a731ca0fc6441d87122197ba763a9c"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:43:52.443022Z","iopub.execute_input":"2025-01-14T17:43:52.443396Z","iopub.status.idle":"2025-01-14T17:43:57.861667Z","shell.execute_reply.started":"2025-01-14T17:43:52.443359Z","shell.execute_reply":"2025-01-14T17:43:57.860924Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.1.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"FinQA_prompt = \"\"\"Below contains texts before table (pre-text), text after the table (post-text) and the table itself with a question that you must answer.\n\n### Pre-text:\n{}\n\n### Table:\n{}\n\n### Post-text:\n{}\n\n### Question:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    \n    pre_texts      = examples[\"pre_text\"]\n    tables         = examples[\"table\"]\n    post_texts     = examples[\"post_text\"]\n    programs = examples[\"expanded_program_re\"]\n    questions      = examples[\"question\"]\n    answers        = examples[\"final_result\"]\n\n    \n    texts = []\n    for pre_text, table, post_text, program, question, answer in zip(pre_texts, tables, post_texts, programs, questions, answers):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = FinQA_prompt.format(pre_text, table, post_text, question, program + ' = ' + answer) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ntrain = load_dataset(\"n3Er/FinQA-Infix\", split = \"train\")\ntest = load_dataset(\"n3Er/FinQA-Infix\", split = \"test\")\nvalidation = load_dataset(\"n3Er/FinQA-Infix\", split = \"validation\")\ntrain = train.map(formatting_prompts_func, batched = True)\n#test = test.map(formatting_prompts_func, batched = True)\n#validation = validation.map(formatting_prompts_func, batched = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:43:57.862779Z","iopub.execute_input":"2025-01-14T17:43:57.863092Z","iopub.status.idle":"2025-01-14T17:44:09.013000Z","shell.execute_reply.started":"2025-01-14T17:43:57.863058Z","shell.execute_reply":"2025-01-14T17:44:09.012081Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.50k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3efe29cecaa4e6793780da9c7ea77b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/12.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8800f65f83174128a9597fe2b79cf735"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11234d5ee34f4f68ba917e0b6b50282e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.19M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"848ea0cb033145c38fda93f7ee6e1618"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/6251 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de929e777a0548509d71b78d45b945cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/883 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ddc8177f7f040de82cee6870dec4b1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1147 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"190f36e6789241f9a0f28a09202103c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6251 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad57f0c134d14056b19a0e5e7fd49af7"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train['final_result']","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-14T17:11:17.556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['text'][194]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-14T17:11:17.556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 3,\n        #remove_unused_columns=False,\n        gradient_accumulation_steps = 6,\n        warmup_steps = 5,\n        num_train_epochs = 1, # Set this for 1 full training run.\n        max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:44:09.013781Z","iopub.execute_input":"2025-01-14T17:44:09.014034Z","iopub.status.idle":"2025-01-14T17:44:19.064717Z","shell.execute_reply.started":"2025-01-14T17:44:09.014008Z","shell.execute_reply":"2025-01-14T17:44:19.063732Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/6251 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"400ad48c361e4ce6af6477e362ffcbe2"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:44:19.065744Z","iopub.execute_input":"2025-01-14T17:44:19.066052Z","iopub.status.idle":"2025-01-14T17:44:19.071759Z","shell.execute_reply.started":"2025-01-14T17:44:19.066027Z","shell.execute_reply":"2025-01-14T17:44:19.070988Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n5.418 GB of memory reserved.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:44:19.072389Z","iopub.execute_input":"2025-01-14T17:44:19.072647Z","iopub.status.idle":"2025-01-14T18:54:19.623588Z","shell.execute_reply.started":"2025-01-14T17:44:19.072615Z","shell.execute_reply":"2025-01-14T18:54:19.622658Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 6,251 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 3 | Gradient Accumulation steps = 6\n\\        /    Total batch size = 18 | Total steps = 60\n \"-____-\"     Number of trainable parameters = 20,971,520\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 1:08:36, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.685700</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.742600</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.826500</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.689700</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.688600</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.678700</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.548700</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.562100</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.595700</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.457100</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.446200</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.447000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.449500</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.429800</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.176800</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.471100</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.496800</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.322600</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.256600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.352100</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.369600</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.350400</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.386200</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.414200</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.364000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.277700</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>1.324300</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.343700</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.236800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.352800</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.382000</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>1.413300</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>1.228900</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>1.383700</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.462700</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>1.143000</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>1.307000</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>1.295400</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>1.325800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.365800</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>1.286800</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>1.301100</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>1.238000</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>1.332500</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.323100</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>1.368100</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>1.348500</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>1.391300</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>1.380900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.392000</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>1.298800</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>1.164900</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>1.296800</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>1.264300</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>1.235300</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>1.441800</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>1.299700</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>1.182500</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>1.265200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.288700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:54:34.448137Z","iopub.execute_input":"2025-01-14T18:54:34.448469Z","iopub.status.idle":"2025-01-14T18:54:34.455316Z","shell.execute_reply.started":"2025-01-14T18:54:34.448443Z","shell.execute_reply":"2025-01-14T18:54:34.454345Z"}},"outputs":[{"name":"stdout","text":"4196.7241 seconds used for training.\n69.95 minutes used for training.\nPeak reserved memory = 8.123 GB.\nPeak reserved memory for training = 2.705 GB.\nPeak reserved memory % of max memory = 55.105 %.\nPeak reserved memory for training % of max memory = 18.35 %.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# pre_text, table, post_text, program, question, answer\n\n# alpaca_prompt = Copied from above\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    FinQA_prompt.format(\n        \"24 2017 annual report performance graph the following chart presents a comparison for the five-year period ended june 30 , 2017 , of the market performance of the company 2019s common stock with the s&p 500 index and an index of peer companies selected by the company : comparison of 5 year cumulative total return among jack henry & associates , inc. , the s&p 500 index , and a peer group the following information depicts a line graph with the following values: .\", # Pre-text\n        \"\"\"\t2012\t2013\t2014\t2015\t2016\t2017\njkhy\t100.00\t138.34\t177.10\t195.72\t267.64\t322.60\npeer group\t100.00\t117.87\t161.90\t203.87\t233.39\t271.10\ns&p 500\t100.00\t120.60\t150.27\t161.43\t167.87\t197.92\"\"\",\n        \"\"\"this comparison assumes $ 100 was invested on june 30 , 2012 , and assumes reinvestments of dividends . total returns are calculated according to market capitalization of peer group members at the beginning of each period . peer companies selected are in the business of providing specialized computer software , hardware and related services to financial institutions and other businesses . companies in the peer group are aci worldwide , inc. ; bottomline technology , inc. ; broadridge financial solutions ; cardtronics , inc. ; convergys corp. ; corelogic , inc. ; dst systems , inc. ; euronet worldwide , inc. ; fair isaac corp. ; fidelity national information services , inc. ; fiserv , inc. ; global payments , inc. ; moneygram international , inc. ; ss&c technologies holdings , inc. ; total systems services , inc. ; tyler technologies , inc. ; verifone systems , inc. ; and wex , inc.. .\"\"\", #post-text\n        \"jkhy's total 5 year return was what percent of the peer group?\",\n        \"\", #responce\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:54:40.857325Z","iopub.execute_input":"2025-01-14T18:54:40.857654Z","iopub.status.idle":"2025-01-14T18:54:42.669558Z","shell.execute_reply.started":"2025-01-14T18:54:40.857629Z","shell.execute_reply":"2025-01-14T18:54:42.668666Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[\"<|begin_of_text|>Below contains texts before table (pre-text), text after the table (post-text) and the table itself with a question that you must answer.\\n\\n### Pre-text:\\n24 2017 annual report performance graph the following chart presents a comparison for the five-year period ended june 30, 2017, of the market performance of the company 2019s common stock with the s&p 500 index and an index of peer companies selected by the company : comparison of 5 year cumulative total return among jack henry & associates, inc., the s&p 500 index, and a peer group the following information depicts a line graph with the following values:.\\n\\n### Table:\\n\\t2012\\t2013\\t2014\\t2015\\t2016\\t2017\\njkhy\\t100.00\\t138.34\\t177.10\\t195.72\\t267.64\\t322.60\\npeer group\\t100.00\\t117.87\\t161.90\\t203.87\\t233.39\\t271.10\\ns&p 500\\t100.00\\t120.60\\t150.27\\t161.43\\t167.87\\t197.92\\n\\n### Post-text:\\nthis comparison assumes $ 100 was invested on june 30, 2012, and assumes reinvestments of dividends. total returns are calculated according to market capitalization of peer group members at the beginning of each period. peer companies selected are in the business of providing specialized computer software, hardware and related services to financial institutions and other businesses. companies in the peer group are aci worldwide, inc. ; bottomline technology, inc. ; broadridge financial solutions ; cardtronics, inc. ; convergys corp. ; corelogic, inc. ; dst systems, inc. ; euronet worldwide, inc. ; fair isaac corp. ; fidelity national information services, inc. ; fiserv, inc. ; global payments, inc. ; moneygram international, inc. ; ss&c technologies holdings, inc. ; total systems services, inc. ; tyler technologies, inc. ; verifone systems, inc. ; and wex, inc...\\n\\n### Question:\\njkhy's total 5 year return was what percent of the peer group?\\n\\n### Response:\\n322.60 / 271.10 = 118.7%<|eot_id|>\"]"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"test['text'][0]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-14T17:11:17.557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = load_dataset(\"n3Er/FinQA-Infix\", split = \"test\")\n\ndef formatting_prompts_test(examples):\n    \n    pre_texts      = examples[\"pre_text\"]\n    tables         = examples[\"table\"]\n    post_texts     = examples[\"post_text\"]\n    programs       = examples[\"expanded_program_re\"]\n    questions      = examples[\"question\"]\n    answers        = examples[\"final_result\"]\n\n    \n    texts = []\n    responses = []\n    for pre_text, table, post_text, program, question, answer in zip(pre_texts, tables, post_texts, programs, questions, answers):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = FinQA_prompt.format(pre_text, table, post_text, question, \"\")\n        True_response = program + ' = ' + answer\n        texts.append(text)\n        responses.append(True_response)\n    return { \"text\" : texts, \"true_responses\" : responses,}\n\ntest = test.map(formatting_prompts_test, batched = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:55:37.791388Z","iopub.execute_input":"2025-01-14T18:55:37.791685Z","iopub.status.idle":"2025-01-14T18:55:38.928990Z","shell.execute_reply.started":"2025-01-14T18:55:37.791662Z","shell.execute_reply":"2025-01-14T18:55:38.927677Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1147 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b91b038040e47ddb6d5b3ab03e41e1a"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"print(test[\"text\"][0])\n#print(test[\"true_responses\"][0])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-14T17:11:17.557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)\n\ntest_input = tokenizer(test[\"text\"][1], return_tensors = \"pt\").to(\"cuda\")\noutputs = model.generate(**test_input, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:55:59.538176Z","iopub.execute_input":"2025-01-14T18:55:59.538499Z","iopub.status.idle":"2025-01-14T18:56:01.742176Z","shell.execute_reply.started":"2025-01-14T18:55:59.538475Z","shell.execute_reply":"2025-01-14T18:56:01.741424Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[\"<|begin_of_text|>Below contains texts before table (pre-text), text after the table (post-text) and the table itself with a question that you must answer.\\n\\n### Pre-text:\\n['item 1b.', 'unresolved staff comments not applicable.', 'item 2.', 'properties as of december 26, 2015, our major facilities consisted of : ( square feet in millions ) united states countries total owned facilities1.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '30.7 17.2 47.9 leased facilities2.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '2.1 6.0 8.1.']\\n\\n### Table:\\n[['( square feet in millions )', 'unitedstates', 'othercountries', 'total'], ['owned facilities1', '30.7', '17.2', '47.9'], ['leased facilities2', '2.1', '6.0', '8.1'], ['total facilities', '32.8', '23.2', '56.0']]\\n\\n### Post-text:\\n['1 leases on portions of the land used for these facilities expire on varying dates through 2062.', '2 leases expire on varying dates through 2030 and generally include renewals at our option.', 'our principal executive offices are located in the u.s.', 'and a majority of our wafer fabrication activities are also located in the u.s.', 'we completed construction of development fabrication facilities in oregon during 2014 that we expect will enable us to maintain our process technology lead.', 'we also completed construction of a large-scale fabrication building in arizona in 2013.', 'a portion of the new oregon and arizona facilities are currently not in use and we are reserving the new buildings for additional capacity and future technologies.', 'incremental construction and equipment installation are required to ready the facilities for their intended use.', 'our massachusetts fabrication facility was our last manufacturing facility on 200mm wafers and ceased production in q1 2015.', 'outside the u.s., we have wafer fabrication facilities in ireland, israel, and china.', 'our fabrication facility in ireland has transitioned to our 14nm process technology, with manufacturing continuing to ramp in 2016.', 'additionally, in the second half of 2016, we will start using our facility in dalian, china to help expand our manufacturing capacity in next-generation memory.', 'our assembly and test facilities are located in malaysia, china, and vietnam.', 'in addition, we have sales and marketing offices worldwide that are generally located near major concentrations of customers.', 'we believe that the facilities described above are suitable and adequate for our present purposes and that the productive capacity in our facilities is substantially being utilized or we have plans to utilize it.', 'we do not identify or allocate assets by operating segment.', 'for information on net property, plant and equipment by country, see 201cnote 26 : operating segments and geographic information 201d in part ii, item 8 of this form 10-k.', 'item 3.', 'legal proceedings for a discussion of legal proceedings, see 201cnote 25 : contingencies 201d in part ii, item 8 of this form 10-k.', 'item 4.','mine safety disclosures not applicable..']\\n\\n### Question:\\nwhat percentage of total facilities as measured in square feet are leased?\\n\\n### Response:\\n(2.1 + 6.0) / 56.0 = 0.07<|eot_id|>\"]"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import re\n\ntest_output = []\n\n\n\ndef extract_response(text):\n    pattern = r\"Response:\\n(.*?)<\\|eot_id\\|>\"\n    match = re.search(pattern, text)\n    if match:\n        return match.group(1)\n    return None\n\n\nfor i in range(len(test[\"text\"])):\n    FastLanguageModel.for_inference(model)\n    test_input = tokenizer(test[\"text\"][i], return_tensors = \"pt\").to(\"cuda\")\n    outputs = model.generate(**test_input, max_new_tokens = 64, use_cache = True)\n    decoded_output = tokenizer.batch_decode(outputs)\n    test_output.append(extract_response(decoded_output[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:56:15.213808Z","iopub.execute_input":"2025-01-14T18:56:15.214294Z","iopub.status.idle":"2025-01-14T19:34:54.708680Z","shell.execute_reply.started":"2025-01-14T18:56:15.214258Z","shell.execute_reply":"2025-01-14T19:34:54.707706Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"test_output[6]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:35:23.998625Z","iopub.execute_input":"2025-01-14T19:35:23.998959Z","iopub.status.idle":"2025-01-14T19:35:24.004051Z","shell.execute_reply.started":"2025-01-14T19:35:23.998929Z","shell.execute_reply":"2025-01-14T19:35:24.003286Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'463 / 4612 = 10.0%'"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"true_test_output = test[\"true_responses\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:35:35.400389Z","iopub.execute_input":"2025-01-14T19:35:35.400705Z","iopub.status.idle":"2025-01-14T19:35:35.405922Z","shell.execute_reply.started":"2025-01-14T19:35:35.400676Z","shell.execute_reply":"2025-01-14T19:35:35.405092Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"true_test_output[6]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:35:36.696451Z","iopub.execute_input":"2025-01-14T19:35:36.696743Z","iopub.status.idle":"2025-01-14T19:35:36.701632Z","shell.execute_reply.started":"2025-01-14T19:35:36.696721Z","shell.execute_reply":"2025-01-14T19:35:36.700744Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'463 / 4612 = 10%'"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import pickle\nwith open(\"test_output.pkl\", 'wb') as file:\n    pickle.dump(test_output, file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:34:54.709747Z","iopub.execute_input":"2025-01-14T19:34:54.710052Z","iopub.status.idle":"2025-01-14T19:34:54.714849Z","shell.execute_reply.started":"2025-01-14T19:34:54.710015Z","shell.execute_reply":"2025-01-14T19:34:54.713960Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import pickle\nwith open(\"true_test_output.pkl\", 'wb') as file:\n    pickle.dump(true_test_output, file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:35:43.389106Z","iopub.execute_input":"2025-01-14T19:35:43.389411Z","iopub.status.idle":"2025-01-14T19:35:43.393578Z","shell.execute_reply.started":"2025-01-14T19:35:43.389388Z","shell.execute_reply":"2025-01-14T19:35:43.392791Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"!pip install rouge-score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:35:46.660857Z","iopub.execute_input":"2025-01-14T19:35:46.661187Z","iopub.status.idle":"2025-01-14T19:35:52.140503Z","shell.execute_reply.started":"2025-01-14T19:35:46.661161Z","shell.execute_reply":"2025-01-14T19:35:52.139542Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=e0306b6f3724149647ff0c0953c0bcfc63717645e647d839cd59dcf34e69a555\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\n# Initialize the ROUGE scorer\nscorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n\nscores = []\n\n\nscores = []\nattribute_error_count = 0\n\ntry:\n    for ref, hyp in zip(true_test_output, test_output):\n        try:\n            score = scorer.score(ref, hyp)\n            scores.append(score)\n        except AttributeError as e:\n            print(f\"An AttributeError occurred: {e}\")\n            attribute_error_count += 1\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n\n# Print the total number of AttributeError exceptions\nprint(f\"Total number of AttributeError exceptions: {attribute_error_count}\")\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:36:01.173512Z","iopub.execute_input":"2025-01-14T19:36:01.173825Z","iopub.status.idle":"2025-01-14T19:36:02.397224Z","shell.execute_reply.started":"2025-01-14T19:36:01.173798Z","shell.execute_reply":"2025-01-14T19:36:02.396490Z"}},"outputs":[{"name":"stdout","text":"An AttributeError occurred: 'NoneType' object has no attribute 'lower'\nAn AttributeError occurred: 'NoneType' object has no attribute 'lower'\nTotal number of AttributeError exceptions: 2\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"suum1 = 0\nsuum2 = 0\nsuum3 = 0\n\nfor i in range(len(scores)):\n    suum1 += float(scores[i]['rougeL'][0])\n    suum2 += float(scores[i]['rougeL'][1])\n    suum3 += float(scores[i]['rougeL'][2])\n\nprecision = suum1 / len(scores)\nrecall = suum2 / len(scores)\nfmeasure = suum3 / len(scores)\nprint(\"The Precision (Rouge-L): {0:.2f}\".format(precision))\nprint(\"The Recall (Rouge-L):    {0:.2f}\".format(recall))\nprint(\"The F-Measure (Rouge-L): {0:.2f}\".format(fmeasure))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:40:35.101418Z","iopub.execute_input":"2025-01-14T19:40:35.101743Z","iopub.status.idle":"2025-01-14T19:40:35.108870Z","shell.execute_reply.started":"2025-01-14T19:40:35.101714Z","shell.execute_reply":"2025-01-14T19:40:35.108167Z"}},"outputs":[{"name":"stdout","text":"The Precision (Rouge-L): 0.64\nThe Recall (Rouge-L):    0.63\nThe F-Measure (Rouge-L): 0.62\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"indexes = []\nfor i in range(len(scores)):\n    f_val = float(scores[i]['rougeL'][2])\n    if f_val < 0.4:\n        indexes.append(i)\n\nprint(len(indexes))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:36:24.677945Z","iopub.execute_input":"2025-01-14T19:36:24.678278Z","iopub.status.idle":"2025-01-14T19:36:24.683868Z","shell.execute_reply.started":"2025-01-14T19:36:24.678254Z","shell.execute_reply":"2025-01-14T19:36:24.683054Z"}},"outputs":[{"name":"stdout","text":"295\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"rougeL_fmeasure = []\n\nfor i in range(len(scores)):\n    f_val = float(scores[i]['rougeL'][2])\n    rougeL_fmeasure.append(f_val)\n\nprint(rougeL_fmeasure.index(min(rougeL_fmeasure)))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-14T17:11:17.558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\nwith open(\"/kaggle/input/the-finetuned-llama-output/test_output.pkl\", 'rb') as file:\n    test_output = pickle.load(file)\nwith open(\"/kaggle/input/the-finetuned-llama-output/true_test_output.pkl\", 'rb') as file:\n    true_test_output = pickle.load(file)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-14T17:11:17.558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(test_output)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-14T17:11:17.558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = 675\nprint(\"The F-measure score for this responce is: \", scores[n]['rougeL'])\nprint(\"The true responce is:                     \", true_test_output[n])\nprint(\"The model output is:                      \", test_output[n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:36:47.917895Z","iopub.execute_input":"2025-01-14T19:36:47.918237Z","iopub.status.idle":"2025-01-14T19:36:47.923665Z","shell.execute_reply.started":"2025-01-14T19:36:47.918201Z","shell.execute_reply":"2025-01-14T19:36:47.922970Z"}},"outputs":[{"name":"stdout","text":"The F-measure score for this responce is:  Score(precision=1.0, recall=1.0, fmeasure=1.0)\nThe true responce is:                      47162 / 83659 = 56%\nThe model output is:                       47162 / 83659 = 56.3%\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"n = indexes[3]\nprint(\"The F-measure score for this responce is: \", scores[n]['rougeL'])\nprint(\"The true responce is:                     \", true_test_output[n])\nprint(\"The model output is:                      \", test_output[n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:36:58.885951Z","iopub.execute_input":"2025-01-14T19:36:58.886264Z","iopub.status.idle":"2025-01-14T19:36:58.892211Z","shell.execute_reply.started":"2025-01-14T19:36:58.886240Z","shell.execute_reply":"2025-01-14T19:36:58.891347Z"}},"outputs":[{"name":"stdout","text":"The F-measure score for this responce is:  Score(precision=0.0, recall=0.0, fmeasure=0.0)\nThe true responce is:                      table_average(net change for the year) = 3298\nThe model output is:                       13928 / 3 = 4660\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"n = 963\nprint(\"The F-measure score for this responce is: \", scores[n]['rougeL'])\nprint(\"The true responce is:                     \", true_test_output[n])\nprint(\"The model output is:                      \", test_output[n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:37:28.168462Z","iopub.execute_input":"2025-01-14T19:37:28.168757Z","iopub.status.idle":"2025-01-14T19:37:28.174659Z","shell.execute_reply.started":"2025-01-14T19:37:28.168734Z","shell.execute_reply":"2025-01-14T19:37:28.173748Z"}},"outputs":[{"name":"stdout","text":"The F-measure score for this responce is:  Score(precision=0.25, recall=0.25, fmeasure=0.25)\nThe true responce is:                      (6569200 * 4.55) / const_1000000 = 29.9\nThe model output is:                       6569.2 / 4.55 = 1443.9\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}