{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install unsloth\n# Also get the latest nightly Unsloth!\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:48:44.624128Z","iopub.execute_input":"2025-01-23T14:48:44.624517Z","iopub.status.idle":"2025-01-23T14:49:17.534816Z","shell.execute_reply.started":"2025-01-23T14:48:44.624487Z","shell.execute_reply":"2025-01-23T14:49:17.533620Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip index versions unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:49:17.536331Z","iopub.execute_input":"2025-01-23T14:49:17.536598Z","iopub.status.idle":"2025-01-23T14:49:18.470068Z","shell.execute_reply.started":"2025-01-23T14:49:17.536576Z","shell.execute_reply":"2025-01-23T14:49:18.469058Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: pip index is currently an experimental command. It may be removed/changed in a future release without prior warning.\u001b[0m\u001b[33m\n\u001b[0munsloth (2025.1.6)\nAvailable versions: 2025.1.6, 2025.1.5, 2025.1.4, 2025.1.3, 2025.1.2, 2025.1.1, 2024.12.12, 2024.12.11, 2024.12.10, 2024.12.9, 2024.12.8, 2024.12.7, 2024.12.6, 2024.12.5, 2024.12.4, 2024.12.3, 2024.12.2, 2024.12.1, 2024.11.11, 2024.11.10, 2024.11.9, 2024.11.8, 2024.11.7, 2024.11.6, 2024.11.5, 2024.11.4, 2024.11.2, 2024.10.7, 2024.10.6, 2024.10.5, 2024.10.4, 2024.10.2, 2024.10.1, 2024.10.0, 2024.9.post4, 2024.9.post3, 2024.9.post2, 2024.9.post1, 2024.9, 2024.8\n  INSTALLED: 2025.1.7\n  LATEST:    2025.1.6\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/gemma-2-9b-bnb-4bit\",\n    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:49:18.471927Z","iopub.execute_input":"2025-01-23T14:49:18.472191Z","iopub.status.idle":"2025-01-23T14:50:07.792515Z","shell.execute_reply.started":"2025-01-23T14:49:18.472169Z","shell.execute_reply":"2025-01-23T14:50:07.791817Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.1.7: Fast Llama patching. Transformers: 4.48.1.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23383b5eab854a0fa38164463afbba0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb21920a8e15482f85f8e1343bbeee6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56eaf0ad7375475c97a4cbf2cabb4fd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3438ecb0480b4b59899709b61cddfada"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/340 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff7afbb971d47299b100a9c5a0b0401"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:50:07.793802Z","iopub.execute_input":"2025-01-23T14:50:07.794132Z","iopub.status.idle":"2025-01-23T14:50:13.933440Z","shell.execute_reply.started":"2025-01-23T14:50:07.794100Z","shell.execute_reply":"2025-01-23T14:50:13.932384Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.1.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"FinQA_prompt = \"\"\"Below contains texts before table (pre-text), text after the table (post-text) and the table itself with a question that you must answer.\n\n### Pre-text:\n{}\n\n### Table:\n{}\n\n### Post-text:\n{}\n\n### Question:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    \n    pre_texts      = examples[\"pre_text\"]\n    tables         = examples[\"table\"]\n    post_texts     = examples[\"post_text\"]\n    programs       = examples[\"program_re\"]\n    questions      = examples[\"question\"]\n    answers        = examples[\"answer\"]\n\n    \n    texts = []\n    for pre_text, table, post_text, program, question, answer in zip(pre_texts, tables, post_texts, programs, questions, answers):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = FinQA_prompt.format(pre_text, table, post_text, question, program + ' = ' + answer) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ntrain = load_dataset(\"ibm/finqa\", split = \"train\", trust_remote_code=True)\ntest = load_dataset(\"ibm/finqa\", split = \"test\", trust_remote_code=True)\nvalidation = load_dataset(\"ibm/finqa\", split = \"validation\", trust_remote_code=True)\ntrain = train.map(formatting_prompts_func, batched = True)\n#test = test.map(formatting_prompts_func, batched = True)\n#validation = validation.map(formatting_prompts_func, batched = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:50:13.934247Z","iopub.execute_input":"2025-01-23T14:50:13.934562Z","iopub.status.idle":"2025-01-23T14:50:22.928795Z","shell.execute_reply.started":"2025-01-23T14:50:13.934533Z","shell.execute_reply":"2025-01-23T14:50:22.927880Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.00k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0591632d54294fbaa23d7a56d550bce5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"finqa.py:   0%|          | 0.00/4.49k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8289eab1f20f46afb44e1470e29e6afc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21c28f6bfde143729fd149c1f782bd74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2e641140dcc4a898b061afd35d666f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d34ea972a074029b6bd7587d3d86aea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66101b0cec6246878e6f7a4987af0d2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6251 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56973b8087414d5faeb90595636ef68b"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"len(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:50:22.929717Z","iopub.execute_input":"2025-01-23T14:50:22.929945Z","iopub.status.idle":"2025-01-23T14:50:22.934854Z","shell.execute_reply.started":"2025-01-23T14:50:22.929926Z","shell.execute_reply":"2025-01-23T14:50:22.934082Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"6251"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)\n\nsamp = [\"\"\"\n\n### User prompt:\nHi, what are you?\n\n### responce:\n\"\"\"]\n\ntest_input = tokenizer(samp, return_tensors = \"pt\").to(\"cuda\")\noutputs = model.generate(**test_input, max_new_tokens = 64, use_cache = True, temperature = 1e-10)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 3,\n        #remove_unused_columns=False,\n        gradient_accumulation_steps = 6,\n        warmup_steps = 5,\n        num_train_epochs = 1, # Set this for 1 full training run.\n        max_steps = 350,\n        learning_rate = 1e-3,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:50:22.935683Z","iopub.execute_input":"2025-01-23T14:50:22.935977Z","iopub.status.idle":"2025-01-23T14:50:35.710251Z","shell.execute_reply.started":"2025-01-23T14:50:22.935947Z","shell.execute_reply":"2025-01-23T14:50:35.709474Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/6251 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"199e86402c61401f93d6a553493732dd"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Show current memory stats","metadata":{}},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:50:35.712992Z","iopub.execute_input":"2025-01-23T14:50:35.713265Z","iopub.status.idle":"2025-01-23T14:50:35.718700Z","shell.execute_reply.started":"2025-01-23T14:50:35.713241Z","shell.execute_reply":"2025-01-23T14:50:35.717739Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n5.496 GB of memory reserved.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:50:35.720515Z","iopub.execute_input":"2025-01-23T14:50:35.720718Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 6,251 | Num Epochs = 2\nO^O/ \\_/ \\    Batch size per device = 3 | Gradient Accumulation steps = 6\n\\        /    Total batch size = 18 | Total steps = 350\n \"-____-\"     Number of trainable parameters = 41,943,040\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='292' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [292/350 6:12:47 < 1:14:33, 0.01 it/s, Epoch 0.84/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.706600</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.753400</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.799400</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.610200</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.553800</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.475200</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.381400</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.399700</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.457900</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.331500</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.326200</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.363700</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.362900</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.342400</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.111500</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.385000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.400900</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.246400</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.179900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.261000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.283100</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.253900</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.317100</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.315300</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.260800</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.212200</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>1.210500</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.262300</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.131700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.261700</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.310800</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>1.314400</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>1.147500</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>1.307600</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.402600</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>1.066000</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>1.229600</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>1.174400</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>1.238600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.299000</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>1.194100</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>1.224300</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>1.152800</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>1.165200</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.250500</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>1.234500</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>1.237300</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>1.265800</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>1.296700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.245500</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>1.198000</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>1.070900</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>1.163300</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>1.143800</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>1.081900</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>1.252100</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>1.176400</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>1.060700</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>1.107700</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.204800</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>1.063500</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>1.004700</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.960500</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>1.055500</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>1.118300</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>1.180200</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>1.020800</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>1.037600</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>1.158600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.041900</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>1.072600</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>1.027600</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>1.221800</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>1.016300</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.144800</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>1.077000</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>1.068800</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>1.039200</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>1.185700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.154100</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>1.229300</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>1.034900</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>1.144600</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>1.103300</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>1.067800</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.949500</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>1.062000</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>1.089800</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.958900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.163200</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>1.203400</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>0.961400</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>0.990800</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>1.007200</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>1.005100</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.963900</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>1.225700</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>1.110500</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>1.046900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.096600</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>1.164600</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>1.118200</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>1.005500</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>0.997100</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.915500</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>1.141600</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>0.981600</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>1.063100</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>1.041200</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.101500</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>0.995900</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>1.018100</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>0.866700</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>1.003300</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>1.089900</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>1.147900</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>1.181400</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>1.080000</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>0.836800</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.023800</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>1.081100</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>0.911000</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>0.999700</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>1.007900</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.000100</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>0.884100</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>1.061600</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>1.008400</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>0.964000</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.057400</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>0.959200</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>0.942200</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>1.027100</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>0.892600</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>1.072300</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>0.854700</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>1.004900</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>0.991000</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>1.071600</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.840800</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>0.833400</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>0.968300</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>1.042100</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>1.083800</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>0.922000</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>1.038100</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>0.988300</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>0.877100</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>0.735900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.933500</td>\n    </tr>\n    <tr>\n      <td>151</td>\n      <td>0.918700</td>\n    </tr>\n    <tr>\n      <td>152</td>\n      <td>0.721100</td>\n    </tr>\n    <tr>\n      <td>153</td>\n      <td>0.867500</td>\n    </tr>\n    <tr>\n      <td>154</td>\n      <td>0.860700</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>0.774100</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>0.956700</td>\n    </tr>\n    <tr>\n      <td>157</td>\n      <td>1.043800</td>\n    </tr>\n    <tr>\n      <td>158</td>\n      <td>1.089700</td>\n    </tr>\n    <tr>\n      <td>159</td>\n      <td>0.989000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.864100</td>\n    </tr>\n    <tr>\n      <td>161</td>\n      <td>0.957300</td>\n    </tr>\n    <tr>\n      <td>162</td>\n      <td>0.906400</td>\n    </tr>\n    <tr>\n      <td>163</td>\n      <td>0.794700</td>\n    </tr>\n    <tr>\n      <td>164</td>\n      <td>0.828800</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>0.874900</td>\n    </tr>\n    <tr>\n      <td>166</td>\n      <td>0.839800</td>\n    </tr>\n    <tr>\n      <td>167</td>\n      <td>0.871600</td>\n    </tr>\n    <tr>\n      <td>168</td>\n      <td>0.965600</td>\n    </tr>\n    <tr>\n      <td>169</td>\n      <td>0.917600</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.866700</td>\n    </tr>\n    <tr>\n      <td>171</td>\n      <td>0.863100</td>\n    </tr>\n    <tr>\n      <td>172</td>\n      <td>0.837200</td>\n    </tr>\n    <tr>\n      <td>173</td>\n      <td>0.848300</td>\n    </tr>\n    <tr>\n      <td>174</td>\n      <td>0.750100</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.763900</td>\n    </tr>\n    <tr>\n      <td>176</td>\n      <td>0.910000</td>\n    </tr>\n    <tr>\n      <td>177</td>\n      <td>0.823300</td>\n    </tr>\n    <tr>\n      <td>178</td>\n      <td>0.976500</td>\n    </tr>\n    <tr>\n      <td>179</td>\n      <td>0.725100</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.728000</td>\n    </tr>\n    <tr>\n      <td>181</td>\n      <td>0.810300</td>\n    </tr>\n    <tr>\n      <td>182</td>\n      <td>0.823000</td>\n    </tr>\n    <tr>\n      <td>183</td>\n      <td>0.825100</td>\n    </tr>\n    <tr>\n      <td>184</td>\n      <td>0.746900</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>0.874600</td>\n    </tr>\n    <tr>\n      <td>186</td>\n      <td>0.808600</td>\n    </tr>\n    <tr>\n      <td>187</td>\n      <td>0.788700</td>\n    </tr>\n    <tr>\n      <td>188</td>\n      <td>0.965500</td>\n    </tr>\n    <tr>\n      <td>189</td>\n      <td>0.704100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.835600</td>\n    </tr>\n    <tr>\n      <td>191</td>\n      <td>0.775100</td>\n    </tr>\n    <tr>\n      <td>192</td>\n      <td>0.874800</td>\n    </tr>\n    <tr>\n      <td>193</td>\n      <td>0.808100</td>\n    </tr>\n    <tr>\n      <td>194</td>\n      <td>0.750200</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>0.920400</td>\n    </tr>\n    <tr>\n      <td>196</td>\n      <td>0.819100</td>\n    </tr>\n    <tr>\n      <td>197</td>\n      <td>0.845200</td>\n    </tr>\n    <tr>\n      <td>198</td>\n      <td>0.931000</td>\n    </tr>\n    <tr>\n      <td>199</td>\n      <td>0.699000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.649900</td>\n    </tr>\n    <tr>\n      <td>201</td>\n      <td>0.797600</td>\n    </tr>\n    <tr>\n      <td>202</td>\n      <td>0.857900</td>\n    </tr>\n    <tr>\n      <td>203</td>\n      <td>0.617600</td>\n    </tr>\n    <tr>\n      <td>204</td>\n      <td>0.849300</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>0.818700</td>\n    </tr>\n    <tr>\n      <td>206</td>\n      <td>0.897900</td>\n    </tr>\n    <tr>\n      <td>207</td>\n      <td>0.882100</td>\n    </tr>\n    <tr>\n      <td>208</td>\n      <td>0.668500</td>\n    </tr>\n    <tr>\n      <td>209</td>\n      <td>0.559800</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.728900</td>\n    </tr>\n    <tr>\n      <td>211</td>\n      <td>0.837200</td>\n    </tr>\n    <tr>\n      <td>212</td>\n      <td>0.884300</td>\n    </tr>\n    <tr>\n      <td>213</td>\n      <td>0.718800</td>\n    </tr>\n    <tr>\n      <td>214</td>\n      <td>0.623400</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>0.884300</td>\n    </tr>\n    <tr>\n      <td>216</td>\n      <td>0.897700</td>\n    </tr>\n    <tr>\n      <td>217</td>\n      <td>0.672200</td>\n    </tr>\n    <tr>\n      <td>218</td>\n      <td>0.767900</td>\n    </tr>\n    <tr>\n      <td>219</td>\n      <td>0.850600</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.848200</td>\n    </tr>\n    <tr>\n      <td>221</td>\n      <td>0.882200</td>\n    </tr>\n    <tr>\n      <td>222</td>\n      <td>0.822700</td>\n    </tr>\n    <tr>\n      <td>223</td>\n      <td>0.747500</td>\n    </tr>\n    <tr>\n      <td>224</td>\n      <td>0.754500</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.848000</td>\n    </tr>\n    <tr>\n      <td>226</td>\n      <td>0.524700</td>\n    </tr>\n    <tr>\n      <td>227</td>\n      <td>0.651400</td>\n    </tr>\n    <tr>\n      <td>228</td>\n      <td>0.659600</td>\n    </tr>\n    <tr>\n      <td>229</td>\n      <td>0.842500</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.681800</td>\n    </tr>\n    <tr>\n      <td>231</td>\n      <td>0.701700</td>\n    </tr>\n    <tr>\n      <td>232</td>\n      <td>0.726100</td>\n    </tr>\n    <tr>\n      <td>233</td>\n      <td>0.674600</td>\n    </tr>\n    <tr>\n      <td>234</td>\n      <td>0.709600</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>0.733900</td>\n    </tr>\n    <tr>\n      <td>236</td>\n      <td>0.597700</td>\n    </tr>\n    <tr>\n      <td>237</td>\n      <td>0.905500</td>\n    </tr>\n    <tr>\n      <td>238</td>\n      <td>0.735600</td>\n    </tr>\n    <tr>\n      <td>239</td>\n      <td>0.841200</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.549800</td>\n    </tr>\n    <tr>\n      <td>241</td>\n      <td>0.711900</td>\n    </tr>\n    <tr>\n      <td>242</td>\n      <td>0.749200</td>\n    </tr>\n    <tr>\n      <td>243</td>\n      <td>0.744500</td>\n    </tr>\n    <tr>\n      <td>244</td>\n      <td>0.938800</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>0.661500</td>\n    </tr>\n    <tr>\n      <td>246</td>\n      <td>0.751900</td>\n    </tr>\n    <tr>\n      <td>247</td>\n      <td>0.878300</td>\n    </tr>\n    <tr>\n      <td>248</td>\n      <td>0.613600</td>\n    </tr>\n    <tr>\n      <td>249</td>\n      <td>0.803300</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.711400</td>\n    </tr>\n    <tr>\n      <td>251</td>\n      <td>0.661600</td>\n    </tr>\n    <tr>\n      <td>252</td>\n      <td>0.675600</td>\n    </tr>\n    <tr>\n      <td>253</td>\n      <td>0.472200</td>\n    </tr>\n    <tr>\n      <td>254</td>\n      <td>0.688800</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>0.560700</td>\n    </tr>\n    <tr>\n      <td>256</td>\n      <td>0.795500</td>\n    </tr>\n    <tr>\n      <td>257</td>\n      <td>0.699600</td>\n    </tr>\n    <tr>\n      <td>258</td>\n      <td>0.623200</td>\n    </tr>\n    <tr>\n      <td>259</td>\n      <td>0.799300</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.659400</td>\n    </tr>\n    <tr>\n      <td>261</td>\n      <td>0.729200</td>\n    </tr>\n    <tr>\n      <td>262</td>\n      <td>0.621000</td>\n    </tr>\n    <tr>\n      <td>263</td>\n      <td>0.617200</td>\n    </tr>\n    <tr>\n      <td>264</td>\n      <td>0.631900</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>0.619100</td>\n    </tr>\n    <tr>\n      <td>266</td>\n      <td>0.650500</td>\n    </tr>\n    <tr>\n      <td>267</td>\n      <td>0.726700</td>\n    </tr>\n    <tr>\n      <td>268</td>\n      <td>0.481500</td>\n    </tr>\n    <tr>\n      <td>269</td>\n      <td>0.704300</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.648900</td>\n    </tr>\n    <tr>\n      <td>271</td>\n      <td>0.737700</td>\n    </tr>\n    <tr>\n      <td>272</td>\n      <td>0.490400</td>\n    </tr>\n    <tr>\n      <td>273</td>\n      <td>0.697900</td>\n    </tr>\n    <tr>\n      <td>274</td>\n      <td>0.662300</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.702600</td>\n    </tr>\n    <tr>\n      <td>276</td>\n      <td>0.618400</td>\n    </tr>\n    <tr>\n      <td>277</td>\n      <td>0.672400</td>\n    </tr>\n    <tr>\n      <td>278</td>\n      <td>0.621700</td>\n    </tr>\n    <tr>\n      <td>279</td>\n      <td>0.491000</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.725300</td>\n    </tr>\n    <tr>\n      <td>281</td>\n      <td>0.635000</td>\n    </tr>\n    <tr>\n      <td>282</td>\n      <td>0.787800</td>\n    </tr>\n    <tr>\n      <td>283</td>\n      <td>0.764200</td>\n    </tr>\n    <tr>\n      <td>284</td>\n      <td>0.701000</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>0.697200</td>\n    </tr>\n    <tr>\n      <td>286</td>\n      <td>0.636300</td>\n    </tr>\n    <tr>\n      <td>287</td>\n      <td>0.661800</td>\n    </tr>\n    <tr>\n      <td>288</td>\n      <td>0.586900</td>\n    </tr>\n    <tr>\n      <td>289</td>\n      <td>0.545100</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.483400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = load_dataset(\"ibm/finqa\", split = \"test\", trust_remote_code=True)\n\ndef formatting_prompts_test(examples):\n    \n    pre_texts      = examples[\"pre_text\"]\n    tables         = examples[\"table\"]\n    post_texts     = examples[\"post_text\"]\n    programs = examples[\"program_re\"]\n    questions      = examples[\"question\"]\n    answers        = examples[\"answer\"]\n\n    \n    texts = []\n    responses = []\n    for pre_text, table, post_text, program, question, answer in zip(pre_texts, tables, post_texts, programs, questions, answers):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = FinQA_prompt.format(pre_text, table, post_text, question, \"\")\n        True_response = program + ' = ' + answer\n        texts.append(text)\n        responses.append(True_response)\n    return { \"text\" : texts, \"true_responses\" : responses,}\n\ntest = test.map(formatting_prompts_test, batched = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(test[\"text\"][0])\n#print(test[\"true_responses\"][0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)\n\nprompt = [\"\"\"### pretext:\nCan you calculate this: 3 * 97 / 45 + 34?\n### Response:\n\"\"\"]\n\n\ntest_input = tokenizer(prompt, return_tensors = \"pt\").to(\"cuda\")\noutputs = model.generate(**test_input, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)\n\nprompt = [\"\"\"### pretext:\nCan you explain about what are you able to do?\n### Response:\n\"\"\"]\n\n\ntest_input = tokenizer(prompt, return_tensors = \"pt\").to(\"cuda\")\noutputs = model.generate(**test_input, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\npred_test_output = []\n\n\n\ndef extract_response(text):\n    pattern = r\"Response:\\n(.*?)<\\|eot_id\\|>\"\n    match = re.search(pattern, text)\n    if match:\n        return match.group(1)\n    return None\n\n\nfor i in range(len(test[\"text\"])):\n    FastLanguageModel.for_inference(model)\n    test_input = tokenizer(test[\"text\"][i], return_tensors = \"pt\").to(\"cuda\")\n    outputs = model.generate(**test_input, max_new_tokens = 64, use_cache = True, temperature = 1e-10)\n    decoded_output = tokenizer.batch_decode(outputs)\n    pred_test_output.append(extract_response(decoded_output[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"true_test_output = test[\"true_responses\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nwith open(\"pred_test_output.pkl\", 'wb') as file:\n    pickle.dump(pred_test_output, file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nwith open(\"true_test_output.pkl\", 'wb') as file:\n    pickle.dump(true_test_output, file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nwith open(\"test_prompts.pkl\", 'wb') as file:\n    pickle.dump(test[\"text\"], file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rouge-score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\n# Initialize the ROUGE scorer\nscorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n\nscores = []\n\n\nscores = []\nattribute_error_count = 0\n\ntry:\n    for ref, hyp in zip(true_test_output, pred_test_output):\n        try:\n            score = scorer.score(ref, hyp)\n            scores.append(score)\n        except AttributeError as e:\n            print(f\"An AttributeError occurred: {e}\")\n            attribute_error_count += 1\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n\n# Print the total number of AttributeError exceptions\nprint(f\"Total number of AttributeError exceptions: {attribute_error_count}\")\n\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"suum1 = 0\nsuum2 = 0\nsuum3 = 0\n\nfor i in range(len(scores)):\n    suum1 += float(scores[i]['rougeL'][0])\n    suum2 += float(scores[i]['rougeL'][1])\n    suum3 += float(scores[i]['rougeL'][2])\n\nprecision = suum1 / len(scores)\nrecall = suum2 / len(scores)\nfmeasure = suum3 / len(scores)\nprint(\"The Precision (Rouge-L): {0:.2f}\".format(precision))\nprint(\"The Recall (Rouge-L):    {0:.2f}\".format(recall))\nprint(\"The F-Measure (Rouge-L): {0:.2f}\".format(fmeasure))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"indexes = []\nfor i in range(len(scores)):\n    f_val = float(scores[i]['rougeL'][2])\n    if f_val < 0.4:\n        indexes.append(i)\n\nprint(len(indexes))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rougeL_fmeasure = []\n\nfor i in range(len(scores)):\n    f_val = float(scores[i]['rougeL'][2])\n    rougeL_fmeasure.append(f_val)\n\nprint(rougeL_fmeasure.index(min(rougeL_fmeasure)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading The Outputs and Metric Reports:","metadata":{}},{"cell_type":"code","source":"!pip install rouge-score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\n# Initialize the ROUGE scorer\nscorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n\nscores = []\n\n\nscores = []\nattribute_error_count = 0\n\ntry:\n    for ref, hyp in zip(true_test_output, pred_test_output):\n        try:\n            score = scorer.score(ref, hyp)\n            scores.append(score)\n        except AttributeError as e:\n            print(f\"An AttributeError occurred: {e}\")\n            attribute_error_count += 1\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n\n# Print the total number of AttributeError exceptions\nprint(f\"Total number of AttributeError exceptions: {attribute_error_count}\")\n\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\nwith open(\"/kaggle/input/the-finetuned-llama-output/pred_test_output.pkl\", 'rb') as file:\n    pred_test_output = pickle.load(file)\nwith open(\"/kaggle/input/the-finetuned-llama-output/true_test_output.pkl\", 'rb') as file:\n    true_test_output = pickle.load(file)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(pred_test_output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = 675\nprint(\"The sample index:                         \",n )\nprint(\"The F-measure score for this responce is: \", scores[n]['rougeL'])\nprint(\"The true responce is:                     \", true_test_output[n])\nprint(\"The model output is:                      \", pred_test_output[n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = 3\nprint(\"The sample index:                         \",n )\nprint(\"The F-measure score for this responce is: \", scores[n]['rougeL'])\nprint(\"The true responce is:                     \", true_test_output[n])\nprint(\"The model output is:                      \", pred_test_output[n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = 963\nprint(\"The sample index:                         \",n )\nprint(\"The F-measure score for this responce is: \", scores[n]['rougeL'])\nprint(\"The true responce is:                     \", true_test_output[n])\nprint(\"The model output is:                      \", pred_test_output[n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = 594\nprint(\"The sample index:                         \",n )\nprint(\"The F-measure score for this responce is: \", scores[n]['rougeL'])\nprint(\"The true responce is:                     \", true_test_output[n])\nprint(\"The model output is:                      \", pred_test_output[n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test[\"text\"][594]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = 592\nprint(\"The sample index:                         \",n )\nprint(\"The F-measure score for this responce is: \", scores[n]['rougeL'])\nprint(\"The true responce is:                     \", true_test_output[n])\nprint(\"The model output is:                      \", pred_test_output[n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(test[\"text\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}