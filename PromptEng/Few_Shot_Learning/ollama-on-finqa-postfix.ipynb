{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-18T17:45:41.750759Z",
     "iopub.status.busy": "2024-12-18T17:45:41.750451Z",
     "iopub.status.idle": "2024-12-18T17:47:01.254203Z",
     "shell.execute_reply": "2024-12-18T17:47:01.253263Z",
     "shell.execute_reply.started": "2024-12-18T17:45:41.750729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "pciutils is already the newest version (1:3.7.0-6).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 72 not upgraded.\n",
      "curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n",
      "######################################################################## 100.0%                                                    7.2%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      ">>> NVIDIA GPU installed.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "Collecting lightrag[ollama]\n",
      "  Downloading lightrag-0.1.0b6-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting backoff<3.0.0,>=2.2.1 (from lightrag[ollama])\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.3 in /opt/conda/lib/python3.10/site-packages (from lightrag[ollama]) (3.1.4)\n",
      "Collecting jsonlines<5.0.0,>=4.0.0 (from lightrag[ollama])\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from lightrag[ollama]) (1.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /opt/conda/lib/python3.10/site-packages (from lightrag[ollama]) (1.26.4)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from lightrag[ollama]) (1.0.1)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from lightrag[ollama]) (6.0.2)\n",
      "Collecting tiktoken<0.8.0,>=0.7.0 (from lightrag[ollama])\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.4 in /opt/conda/lib/python3.10/site-packages (from lightrag[ollama]) (4.66.4)\n",
      "Collecting ollama<0.3.0,>=0.2.1 (from lightrag[ollama])\n",
      "  Downloading ollama-0.2.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.3->lightrag[ollama]) (2.1.5)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines<5.0.0,>=4.0.0->lightrag[ollama]) (23.2.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from ollama<0.3.0,>=0.2.1->lightrag[ollama]) (0.27.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.32.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (4.4.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (1.26.18)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (4.12.2)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading ollama-0.2.1-py3-none-any.whl (9.7 kB)\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightrag-0.1.0b6-py3-none-any.whl (159 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.1/159.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jsonlines, backoff, tiktoken, ollama, lightrag\n",
      "Successfully installed backoff-2.2.1 jsonlines-4.0.0 lightrag-0.1.0b6 ollama-0.2.1 tiktoken-0.7.0\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=0357918591566f62ce440d4449653df93700d4d689800c0e4a756a8d2065698f\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!sudo apt-get install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
    "!pip install -U lightrag[ollama]\n",
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T17:47:01.256307Z",
     "iopub.status.busy": "2024-12-18T17:47:01.255983Z",
     "iopub.status.idle": "2024-12-18T17:47:01.329665Z",
     "shell.execute_reply": "2024-12-18T17:47:01.328724Z",
     "shell.execute_reply.started": "2024-12-18T17:47:01.256263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Create a Python script to start the Ollama API server in a separate thread\n",
    "\n",
    "import os\n",
    "import threading\n",
    "import subprocess\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def ollama():\n",
    "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
    "    subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "ollama_thread = threading.Thread(target=ollama)\n",
    "ollama_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T17:47:01.331136Z",
     "iopub.status.busy": "2024-12-18T17:47:01.330799Z",
     "iopub.status.idle": "2024-12-18T17:47:38.670121Z",
     "shell.execute_reply": "2024-12-18T17:47:38.669185Z",
     "shell.execute_reply.started": "2024-12-18T17:47:01.331097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!ollama pull llama3.1:8b\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T17:47:38.672452Z",
     "iopub.status.busy": "2024-12-18T17:47:38.672141Z",
     "iopub.status.idle": "2024-12-18T17:47:39.728457Z",
     "shell.execute_reply": "2024-12-18T17:47:39.727554Z",
     "shell.execute_reply.started": "2024-12-18T17:47:38.672422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/12/18 - 17:47:39 | 200 |      30.274µs |       127.0.0.1 | HEAD     \"/\"\n",
      "[GIN] 2024/12/18 - 17:47:39 | 200 |   24.438165ms |       127.0.0.1 | POST     \"/api/create\"\n",
      "\u001b[?25ltransferring model data \n",
      "using existing layer sha256:667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 \n",
      "using existing layer sha256:948af2743fc78a328dcb3b0f5a31b3d75f415840fdb699e8b1235978392ecf85 \n",
      "using existing layer sha256:0ba8f0e314b4264dfd19df045cde9d4c394a52474bf92ed6a3de22a4ca31a177 \n",
      "creating new layer sha256:6b346e388b04ec0fac477285ecd957b14813d92703eb4c0cb3d02ba564036e69 \n",
      "creating new layer sha256:50a0d6e597d12c01753e3f1530997e590ecafe25f2140bcb1879bdb180586c11 \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama create myLLM -f /kaggle/input/modelfile-postfix/Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T17:47:55.637436Z",
     "iopub.status.busy": "2024-12-18T17:47:55.636767Z",
     "iopub.status.idle": "2024-12-18T17:47:56.655470Z",
     "shell.execute_reply": "2024-12-18T17:47:56.654343Z",
     "shell.execute_reply.started": "2024-12-18T17:47:55.637397Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/12/18 - 17:47:56 | 200 |      40.565µs |       127.0.0.1 | HEAD     \"/\"\n",
      "[GIN] 2024/12/18 - 17:47:56 | 200 |     801.273µs |       127.0.0.1 | GET      \"/api/tags\"\n",
      "NAME            ID              SIZE      MODIFIED       \n",
      "myLLM:latest    c6c4178448c5    4.9 GB    16 seconds ago    \n",
      "llama3.1:8b     46e0c10c039e    4.9 GB    17 seconds ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T17:49:41.951858Z",
     "iopub.status.busy": "2024-12-18T17:49:41.951436Z",
     "iopub.status.idle": "2024-12-18T17:49:42.054485Z",
     "shell.execute_reply": "2024-12-18T17:49:42.053570Z",
     "shell.execute_reply.started": "2024-12-18T17:49:41.951822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from lightrag.core.generator import Generator\n",
    "from lightrag.core.component import Component\n",
    "from lightrag.core.model_client import ModelClient\n",
    "from lightrag.components.model_client import OllamaClient\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "qa_template = r\"\"\"<SYS>\n",
    "You are a helpful assistant. Just answer the question and avoid chatting, just give me the post-fix answer for the question. Just in case you don't find the answer or you are not sure give me this answer: \"Not found!\" .In case the question is not a numerical question this must be your output: \"I only answer to numerical questions; this is I am a numerical assistant; this is irrelevent!\"\n",
    "Below is a set of examples from dataset, learn from them and answer according to the notation and pattern you see.\n",
    "example1: \n",
    "\n",
    "\n",
    "</SYS>\n",
    "User: {{input_str}}\n",
    "You:\"\"\"\n",
    "\n",
    "class SimpleQA(Component):\n",
    "    def __init__(self, model_client: ModelClient, model_kwargs: dict):\n",
    "        super().__init__()\n",
    "        self.generator = Generator(\n",
    "            model_client=model_client,\n",
    "            model_kwargs=model_kwargs,\n",
    "            template=qa_template,\n",
    "        )\n",
    "\n",
    "    def call(self, input: dict) -> str:\n",
    "        return self.generator.call({\"input_str\": str(input)})\n",
    "\n",
    "    async def acall(self, input: dict) -> str:\n",
    "        return await self.generator.acall({\"input_str\": str(input)})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T17:49:43.146715Z",
     "iopub.status.busy": "2024-12-18T17:49:43.145930Z",
     "iopub.status.idle": "2024-12-18T17:49:49.956500Z",
     "shell.execute_reply": "2024-12-18T17:49:49.955512Z",
     "shell.execute_reply.started": "2024-12-18T17:49:43.146679Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-12-18T17:49:43.701Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 gpu=GPU-c8b76643-7a71-2f57-0660-b5f7ed27df64 parallel=1 available=16790978560 required=\"11.1 GiB\"\n",
      "time=2024-12-18T17:49:43.801Z level=INFO source=server.go:104 msg=\"system memory\" total=\"31.4 GiB\" free=\"30.0 GiB\" free_swap=\"0 B\"\n",
      "time=2024-12-18T17:49:43.801Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[15.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"11.1 GiB\" memory.required.partial=\"11.1 GiB\" memory.required.kv=\"4.0 GiB\" memory.required.allocations=\"[11.1 GiB]\" memory.weights.total=\"7.9 GiB\" memory.weights.repeating=\"7.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"2.1 GiB\" memory.graph.partial=\"2.2 GiB\"\n",
      "time=2024-12-18T17:49:43.802Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --ctx-size 32768 --batch-size 512 --n-gpu-layers 33 --threads 2 --parallel 1 --port 40067\"\n",
      "time=2024-12-18T17:49:43.803Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
      "time=2024-12-18T17:49:43.803Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
      "time=2024-12-18T17:49:43.804Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
      "time=2024-12-18T17:49:43.866Z level=INFO source=runner.go:945 msg=\"starting go runner\"\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\n",
      "time=2024-12-18T17:49:43.876Z level=INFO source=runner.go:946 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=2\n",
      "time=2024-12-18T17:49:43.876Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:40067\"\n",
      "llama_load_model_from_file: using device CUDA0 (Tesla P100-PCIE-16GB) - 16013 MiB free\n",
      "llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "time=2024-12-18T17:49:44.056Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading output layer to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
      "llm_load_tensors:        CUDA0 model buffer size =  4403.49 MiB\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 32768\n",
      "llama_new_context_with_model: n_ctx_per_seq = 32768\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (32768) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  4096.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.50 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  2144.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    72.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "time=2024-12-18T17:49:47.069Z level=INFO source=server.go:594 msg=\"llama runner started in 3.27 seconds\"\n",
      "time=2024-12-18T17:49:47.069Z level=WARN source=routes.go:273 msg=\"the context field is deprecated and will be removed in a future version of Ollama\"\n",
      "llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 1\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = all F32\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llama_model_load: vocab only - skipping tensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/12/18 - 17:49:49 | 200 |   6.42253998s |       127.0.0.1 | POST     \"/api/generate\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:** Happiness is a state of being content and fulfilled, often characterized by feelings of joy, satisfaction, and well-being. It can be achieved through various means such as positive relationships, good health, personal growth, and a sense of purpose."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lightrag.components.model_client import OllamaClient\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "\n",
    "model = {\n",
    "    \"model_client\": OllamaClient(),\n",
    "    \"model_kwargs\": {\"model\": \"myLLM\"}\n",
    "}\n",
    "qa = SimpleQA(**model)\n",
    "output=qa(\"what is happiness\")\n",
    "display(Markdown(f\"**Answer:** {output.data}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T17:50:11.611507Z",
     "iopub.status.busy": "2024-12-18T17:50:11.610176Z",
     "iopub.status.idle": "2024-12-18T17:50:12.578104Z",
     "shell.execute_reply": "2024-12-18T17:50:12.577288Z",
     "shell.execute_reply.started": "2024-12-18T17:50:11.611455Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-12-18T17:50:11.640Z level=WARN source=routes.go:273 msg=\"the context field is deprecated and will be removed in a future version of Ollama\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/12/18 - 17:50:12 | 200 |  956.767685ms |       127.0.0.1 | POST     \"/api/generate\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:** Not found!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "other items on our consolidated financial statements have been appropriately adjusted from the amounts provided in the earnings release , including a reduction of our full year 2016 gross profit and income from operations by $ 2.9 million , and a reduction of net income by $ 1.7 million. .\n",
    "( in thousands )\tat december 31 , 2016\tat december 31 , 2015\tat december 31 , 2014\tat december 31 , 2013\tat december 31 , 2012\n",
    "cash and cash equivalents\t$ 250470\t$ 129852\t$ 593175\t$ 347489\t$ 341841\n",
    "working capital ( 1 )\t1279337\t1019953\t1127772\t702181\t651370\n",
    "inventories\t917491\t783031\t536714\t469006\t319286\n",
    "total assets\t3644331\t2865970\t2092428\t1576369\t1155052\n",
    "total debt including current maturities\t817388\t666070\t281546\t151551\t59858\n",
    "total stockholders 2019 equity\t$ 2030900\t$ 1668222\t$ 1350300\t$ 1053354\t$ 816922\n",
    "( 1 ) working capital is defined as current assets minus current liabilities. Question: what was the percentage change in inventories from 2015 to 2016?\n",
    "\n",
    "\"\"\"\n",
    "output=qa(prompt)\n",
    "display(Markdown(f\"**Answer:** {output.data}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:27:25.397277Z",
     "iopub.status.busy": "2024-12-18T16:27:25.396936Z",
     "iopub.status.idle": "2024-12-18T16:27:54.864681Z",
     "shell.execute_reply": "2024-12-18T16:27:54.863831Z",
     "shell.execute_reply.started": "2024-12-18T16:27:25.397238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "convert from infix notation into postfix notation: 8 * (11+43) ÷ 67\n",
    "\"\"\"\n",
    "output=qa(prompt)\n",
    "display(Markdown(f\"**Answer:** {output.data}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:27:54.867111Z",
     "iopub.status.busy": "2024-12-18T16:27:54.866851Z",
     "iopub.status.idle": "2024-12-18T16:28:24.023273Z",
     "shell.execute_reply": "2024-12-18T16:28:24.022438Z",
     "shell.execute_reply.started": "2024-12-18T16:27:54.867085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "(12 * 67) ^ (23) - 152\n",
    "\"\"\"\n",
    "output=qa(prompt)\n",
    "display(Markdown(f\"**Answer:** {output.data}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:28:30.468391Z",
     "iopub.status.busy": "2024-12-18T16:28:30.467693Z",
     "iopub.status.idle": "2024-12-18T16:28:30.472906Z",
     "shell.execute_reply": "2024-12-18T16:28:30.471884Z",
     "shell.execute_reply.started": "2024-12-18T16:28:30.468362Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"The true answer for above:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:28:31.50142Z",
     "iopub.status.busy": "2024-12-18T16:28:31.501088Z",
     "iopub.status.idle": "2024-12-18T16:28:32.822706Z",
     "shell.execute_reply": "2024-12-18T16:28:32.821844Z",
     "shell.execute_reply.started": "2024-12-18T16:28:31.501391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "generated = \"Subtract(12.393, 22.366), divide(#0, 22.366)\"\n",
    "target = \"subtract(22.366, 12.393), divide(#0, 12.393)\"\n",
    "\n",
    "\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = scorer.score(target, generated)\n",
    "\n",
    "print(\"rouge1: \", scores['rouge1'])\n",
    "print(\"rouge2: \", scores['rouge2'])\n",
    "print(\"rougeL: \", scores['rougeL'])\n",
    "\n",
    "\n",
    "# BLEU score calculation\n",
    "bleu_score = sentence_bleu([target], generated)\n",
    "\n",
    "print(f\"BLEU score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:28:32.824343Z",
     "iopub.status.busy": "2024-12-18T16:28:32.824007Z",
     "iopub.status.idle": "2024-12-18T16:28:41.463287Z",
     "shell.execute_reply": "2024-12-18T16:28:41.462475Z",
     "shell.execute_reply.started": "2024-12-18T16:28:32.824317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load FinQA dataset\n",
    "dataset = load_dataset(\"ibm/finqa\", trust_remote_code=True)\n",
    "\n",
    "# Check the structure\n",
    "print(dataset)\n",
    "print(dataset['train'][0])  # View the first training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:28:41.464824Z",
     "iopub.status.busy": "2024-12-18T16:28:41.464555Z",
     "iopub.status.idle": "2024-12-18T16:28:41.470927Z",
     "shell.execute_reply": "2024-12-18T16:28:41.470115Z",
     "shell.execute_reply.started": "2024-12-18T16:28:41.464799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset['train'][10]['program_re']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:28:41.472196Z",
     "iopub.status.busy": "2024-12-18T16:28:41.471953Z",
     "iopub.status.idle": "2024-12-18T16:28:41.538754Z",
     "shell.execute_reply": "2024-12-18T16:28:41.537882Z",
     "shell.execute_reply.started": "2024-12-18T16:28:41.472172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset['train'][10]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:28:41.540869Z",
     "iopub.status.busy": "2024-12-18T16:28:41.540595Z",
     "iopub.status.idle": "2024-12-18T16:28:47.145172Z",
     "shell.execute_reply": "2024-12-18T16:28:47.144394Z",
     "shell.execute_reply.started": "2024-12-18T16:28:41.540834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Authentication token\n",
    "token = \"User Access Token for HuggingFace\"\n",
    "\n",
    "# Initialize tokenizer for Llama 3.1\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", token=token)\n",
    "\n",
    "# Add padding token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Preprocessing function with corrected target handling\n",
    "def preprocess(example):\n",
    "    # Safeguard for missing fields\n",
    "    post_text = example.get('post_text', '')\n",
    "    pre_text = example.get('pre_text', '')\n",
    "    table = example.get('table', '')\n",
    "    question = example.get('question', '')\n",
    "    program = example.get('program_re', '')\n",
    "\n",
    "    # Format input and target text\n",
    "    input_text = f\"post_text: {post_text} table: {table} pre_text: {pre_text} Question: {question}\"\n",
    "    target_text = program  \n",
    "\n",
    "    # Tokenize input and target text\n",
    "    inputs = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=8192, return_tensors=\"pt\")\n",
    "    targets = tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"][0],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"][0],\n",
    "        \"labels_tokenized\": targets[\"input_ids\"][0]  # Tokenized target for training\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:28:47.146741Z",
     "iopub.status.busy": "2024-12-18T16:28:47.146252Z",
     "iopub.status.idle": "2024-12-18T16:29:49.150552Z",
     "shell.execute_reply": "2024-12-18T16:29:49.149544Z",
     "shell.execute_reply.started": "2024-12-18T16:28:47.146712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processed_dataset = dataset.map(preprocess, remove_columns=dataset[\"train\"].column_names, batched=False)\n",
    "\n",
    "# Set PyTorch format for DataLoader, but include `labels` as a string for ROUGE\n",
    "processed_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels_tokenized\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:29:49.152969Z",
     "iopub.status.busy": "2024-12-18T16:29:49.152173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "train_loader = DataLoader(processed_dataset[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "scores = []\n",
    "\n",
    "sample_limit = 128\n",
    "\n",
    "count = 0\n",
    "# Evaluation loop with ROUGE scoring\n",
    "for batch in train_loader:\n",
    "    count += 1\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels_tokenized = batch[\"labels_tokenized\"]\n",
    "\n",
    "\n",
    "    # Decode inputs to strings\n",
    "    input_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
    "    target_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in labels_tokenized]\n",
    "\n",
    "    \n",
    "    # Generate model responses\n",
    "    responses = []\n",
    "    for input_text in input_texts:\n",
    "        try:\n",
    "            response = qa(input_text)\n",
    "            responses.append(response.data)\n",
    "        except Exception as e:\n",
    "            responses.append(\"API_ERROR\")\n",
    "            print(f\"API call failed: {e}\")\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    for target, response in zip(target_texts, responses): \n",
    "        scores.append(scorer.score(target, response))\n",
    "\n",
    "    if sample_limit <= batch_size * count:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "input_str = input_texts[n]\n",
    "response_text = responses[n] \n",
    "targ = target_texts[n]\n",
    "\n",
    "print(\"input:\", input_str)\n",
    "print(\"\\n\\ntarget text:\", targ)\n",
    "print(\"\\n\\nresponce text:\", response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(len(input_texts))\n",
    "print(len(target_texts))\n",
    "print(len(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#n = 22\n",
    "#n = 5\n",
    "#n = 56\n",
    "n = 9\n",
    "\n",
    "input_str = input_texts[n]\n",
    "response_text = responses[n] \n",
    "targ = target_texts[n]\n",
    "\n",
    "print(\"input:\", input_str)\n",
    "print(\"\\n\\ntarget text:\", targ)\n",
    "print(\"\\n\\nresponce text:\", response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T22:33:06.242278Z",
     "iopub.status.busy": "2024-12-17T22:33:06.241929Z",
     "iopub.status.idle": "2024-12-17T22:33:06.247418Z",
     "shell.execute_reply": "2024-12-17T22:33:06.246507Z",
     "shell.execute_reply.started": "2024-12-17T22:33:06.242248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(target_texts)):\n",
    "    print(target_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T22:33:01.704865Z",
     "iopub.status.busy": "2024-12-17T22:33:01.704454Z",
     "iopub.status.idle": "2024-12-17T22:33:01.709833Z",
     "shell.execute_reply": "2024-12-17T22:33:01.708855Z",
     "shell.execute_reply.started": "2024-12-17T22:33:01.70483Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(responses)):\n",
    "    print(responses[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T22:30:19.890271Z",
     "iopub.status.busy": "2024-12-17T22:30:19.890024Z",
     "iopub.status.idle": "2024-12-17T22:30:19.896626Z",
     "shell.execute_reply": "2024-12-17T22:30:19.895745Z",
     "shell.execute_reply.started": "2024-12-17T22:30:19.890246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "suum = 0\n",
    "for i in range(len(scores)):\n",
    "    suum += float(scores[i]['rougeL'][2])\n",
    "print(suum / len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T18:27:03.313032Z",
     "iopub.status.busy": "2024-12-17T18:27:03.312736Z",
     "iopub.status.idle": "2024-12-17T18:27:03.324665Z",
     "shell.execute_reply": "2024-12-17T18:27:03.323976Z",
     "shell.execute_reply.started": "2024-12-17T18:27:03.313005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T18:27:03.326296Z",
     "iopub.status.busy": "2024-12-17T18:27:03.325776Z",
     "iopub.status.idle": "2024-12-17T18:27:03.367228Z",
     "shell.execute_reply": "2024-12-17T18:27:03.366564Z",
     "shell.execute_reply.started": "2024-12-17T18:27:03.326255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define two sentences\n",
    "# sentence1 = \"Subtract(12.393, 22.366), divide(#0, 22.366)\"\n",
    "# sentence2 = \"subtract(22.366, 12.393), divide(#0, 12.393)\"\n",
    "\n",
    "sentence1 = \"The capital of France is Paris.\"\n",
    "sentence2 = \"Paris is not the capital of France.\"\n",
    "\n",
    "\n",
    "# Create a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the sentences into TF-IDF vectors\n",
    "tfidf_matrix = vectorizer.fit_transform([sentence1, sentence2])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "\n",
    "# Print the similarity score\n",
    "print(f\"Cosine Similarity: {similarity[0][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T18:27:03.368912Z",
     "iopub.status.busy": "2024-12-17T18:27:03.368286Z",
     "iopub.status.idle": "2024-12-17T18:27:03.558181Z",
     "shell.execute_reply": "2024-12-17T18:27:03.556941Z",
     "shell.execute_reply.started": "2024-12-17T18:27:03.36887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Correct answer and LLM output\n",
    "correct_answer = \"The capital of France is Paris.\"\n",
    "llm_output = \"Paris is not the capital of France.\"\n",
    "\n",
    "# Generate embeddings\n",
    "correct_embedding = model.encode(correct_answer, convert_to_tensor=True)\n",
    "llm_embedding = model.encode(llm_output, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity = util.cos_sim(correct_embedding, llm_embedding)\n",
    "\n",
    "print(f\"Cosine Similarity: {similarity.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-17T18:27:03.55889Z",
     "iopub.status.idle": "2024-12-17T18:27:03.559173Z",
     "shell.execute_reply": "2024-12-17T18:27:03.559048Z",
     "shell.execute_reply.started": "2024-12-17T18:27:03.559033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6331143,
     "sourceId": 10238124,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
