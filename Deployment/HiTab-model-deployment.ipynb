{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install unsloth\n# Also get the latest nightly Unsloth!\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip index versions unsloth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"BijanProjects/Llama_3.1_8B_IBM_FinQA_FineTuned\", \n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ntest = load_dataset(\"zhoujun/hitab\", split = \"test\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FinQA_prompt = \"\"\"Below contains texts before table (pre-text), text after the table (post-text) and the table itself with a question that you must answer.\n\n### Pre-text:\n\"\"\n\n### Table:\n{}\n\n### Post-text:\n\"\"\n\n### Question:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    \n\n    tables         = examples[\"table\"]\n    questions      = examples[\"question\"]\n    answers        = examples[\"answer\"]\n\n    \n    texts = []\n    for table, question, answer in zip(tables, questions, answers):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = FinQA_prompt.format(table, question, answer[0]) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\n\ntest = load_dataset(\"zhoujun/hitab\", split = \"test\")\ntest = test.map(formatting_prompts_func, batched = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test['text'][0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Show current memory stats","metadata":{}},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Deployment","metadata":{}},{"cell_type":"code","source":"test = load_dataset(\"zhoujun/hitab\", split = \"test\")\n\ndef formatting_prompts_test(examples):\n    \n    tables         = examples[\"table\"]\n    questions      = examples[\"question\"]\n    answers        = examples[\"answer\"]\n\n    \n    texts = []\n    responses = []\n    for table, question, answer in zip(tables, questions, answers):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = FinQA_prompt.format(table, question, \"\")\n        True_response = answer[0]\n        texts.append(text)\n        responses.append(True_response)\n    return { \"text\" : texts, \"true_responses\" : responses,}\n\ntest = test.map(formatting_prompts_test, batched = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(test[\"text\"][0])\nprint(test[\"true_responses\"][0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)\n\nprompt = [\"\"\"### pretext:\nCan you explain about what are you able to do?\n### Response:\n\"\"\"]\n\n\ntest_input = tokenizer(prompt, return_tensors = \"pt\").to(\"cuda\")\noutputs = model.generate(**test_input, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\npred_test_output = []\npred_final_answer = []\n\n\ndef extract_response(text):\n    pattern = r\"Response:\\n(.*?)<\\|eot_id\\|>\"\n    match = re.search(pattern, text)\n    if match:\n        return match.group(1)\n    return None\n\ndef extract_answer(text):\n    pattern = r\"=(.*?)<\\|eot_id\\|>\"\n    match = re.search(pattern, text)\n    if match:\n        return match.group(1)\n    return None\n\n\nfor i in range(len(test[\"text\"])):\n    FastLanguageModel.for_inference(model)\n    test_input = tokenizer(test[\"text\"][i], return_tensors = \"pt\").to(\"cuda\")\n    outputs = model.generate(**test_input, max_new_tokens = 64, use_cache = True, temperature = 1e-10)\n    decoded_output = tokenizer.batch_decode(outputs)\n    pred_test_output.append(extract_response(decoded_output[0]))\n    pred_final_answer.append(extract_answer(decoded_output[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"true_test_output = test[\"true_responses\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nwith open(\"pred_test_output.pkl\", 'wb') as file:\n    pickle.dump(pred_test_output, file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nwith open(\"true_test_output.pkl\", 'wb') as file:\n    pickle.dump(true_test_output, file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# with open(\"true_test_output.pkl\", 'rb') as file:\n    # true_test_output = pickle.load(file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nwith open(\"test_prompts.pkl\", 'wb') as file:\n    pickle.dump(test[\"text\"], file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rouge-score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\n# Initialize the ROUGE scorer\nscorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n\nscores = []\n\n\nscores = []\nattribute_error_count = 0\n\ntry:\n    for ref, hyp in zip(true_test_output, pred_final_answer):\n        try:\n            score = scorer.score(ref, hyp)\n            scores.append(score)\n        except AttributeError as e:\n            print(f\"An AttributeError occurred: {e}\")\n            attribute_error_count += 1\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n\n# Print the total number of AttributeError exceptions\nprint(f\"Total number of AttributeError exceptions: {attribute_error_count}\")\n\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"suum1 = 0\nsuum2 = 0\nsuum3 = 0\n\nfor i in range(len(scores)):\n    suum1 += float(scores[i]['rougeL'][0])\n    suum2 += float(scores[i]['rougeL'][1])\n    suum3 += float(scores[i]['rougeL'][2])\n\nprecision = suum1 / len(scores)\nrecall = suum2 / len(scores)\nfmeasure = suum3 / len(scores)\nprint(\"The Precision (Rouge-L): {0:.2f}\".format(precision))\nprint(\"The Recall (Rouge-L):    {0:.2f}\".format(recall))\nprint(\"The F-Measure (Rouge-L): {0:.2f}\".format(fmeasure))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"indexes = []\nfor i in range(len(scores)):\n    f_val = float(scores[i]['rougeL'][2])\n    if f_val < 0.4:\n        indexes.append(i)\n\nprint(len(indexes))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rougeL_fmeasure = []\n\nfor i in range(len(scores)):\n    f_val = float(scores[i]['rougeL'][2])\n    rougeL_fmeasure.append(f_val)\n\nprint(rougeL_fmeasure.index(min(rougeL_fmeasure)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading The Outputs and Metric Reports:","metadata":{}},{"cell_type":"code","source":"!pip install rouge-score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\n# Initialize the ROUGE scorer\nscorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n\nscores = []\n\n\nscores = []\nattribute_error_count = 0\n\ntry:\n    for ref, hyp in zip(true_test_output, pred_test_output):\n        try:\n            score = scorer.score(ref, hyp)\n            scores.append(score)\n        except AttributeError as e:\n            print(f\"An AttributeError occurred: {e}\")\n            attribute_error_count += 1\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n\n# Print the total number of AttributeError exceptions\nprint(f\"Total number of AttributeError exceptions: {attribute_error_count}\")\n\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\nwith open(\"/kaggle/input/the-finetuned-llama-output/pred_test_output.pkl\", 'rb') as file:\n    pred_test_output = pickle.load(file)\nwith open(\"/kaggle/input/the-finetuned-llama-output/true_test_output.pkl\", 'rb') as file:\n    true_test_output = pickle.load(file)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(pred_test_output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = 675\nprint(\"The sample index:                         \",n )\nprint(\"The F-measure score for this responce is: \", scores[n]['rougeL'])\nprint(\"The true responce is:                     \", true_test_output[n])\nprint(\"The model output is:                      \", pred_test_output[n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = 3\nprint(\"The sample index:                         \",n )\nprint(\"The F-measure score for this responce is: \", scores[n]['rougeL'])\nprint(\"The true responce is:                     \", true_test_output[n])\nprint(\"The model output is:                      \", pred_test_output[n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = 963\nprint(\"The sample index:                         \",n )\nprint(\"The F-measure score for this responce is: \", scores[n]['rougeL'])\nprint(\"The true responce is:                     \", true_test_output[n])\nprint(\"The model output is:                      \", pred_test_output[n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = 594\nprint(\"The sample index:                         \",n )\nprint(\"The F-measure score for this responce is: \", scores[n]['rougeL'])\nprint(\"The true responce is:                     \", true_test_output[n])\nprint(\"The model output is:                      \", pred_test_output[n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test[\"text\"][22]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = 22\nprint(\"The sample index:                         \",n )\nprint(\"The F-measure score for this responce is: \", scores[n]['rougeL'])\nprint(\"The true responce is:                     \", true_test_output[n])\nprint(\"The model output is:                      \", pred_test_output[n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(test[\"text\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"true_test_output[1000]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\n# Sample lists\n# true_test_output \nlist_1 = pred_test_output\nlist_2 = pred_final_answer\n\n# Function to check if a string contains a numerical value\ndef contains_number(s):\n    return bool(re.search(r'\\d', s))\n\n# Get boolean mask for numerical values\nis_numeric = [contains_number(item) for item in true_test_output]\n\n# Filtering lists based on numeric values\nfiltered_list_1_numeric = [val for val, flag in zip(list_1, is_numeric) if flag]\nfiltered_list_2_numeric = [val for val, flag in zip(list_2, is_numeric) if flag]\nfiltered_true_test_output_numeric = [val for val, flag in zip(true_test_output, is_numeric) if flag]\n\n# Display results\nprint(\"Filtered Numeric True Test Output:\", filtered_true_test_output_numeric)\nprint(\"Filtered Numeric List 1:\", filtered_list_1_numeric)\nprint(\"Filtered Numeric List 2:\", filtered_list_2_numeric)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\n# Initialize the ROUGE scorer\nscorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n\nscores = []\n\n\nscores = []\nattribute_error_count = 0\n\ntry:\n    for ref, hyp in zip(filtered_true_test_output_numeric, filtered_list_2_numeric):\n        try:\n            score = scorer.score(ref, hyp)\n            scores.append(score)\n        except AttributeError as e:\n            print(f\"An AttributeError occurred: {e}\")\n            attribute_error_count += 1\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n\n# Print the total number of AttributeError exceptions\nprint(f\"Total number of AttributeError exceptions: {attribute_error_count}\")\n\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"suum1 = 0\nsuum2 = 0\nsuum3 = 0\n\nfor i in range(len(scores)):\n    suum1 += float(scores[i]['rougeL'][0])\n    suum2 += float(scores[i]['rougeL'][1])\n    suum3 += float(scores[i]['rougeL'][2])\n\nprecision = suum1 / len(scores)\nrecall = suum2 / len(scores)\nfmeasure = suum3 / len(scores)\nprint(\"The Precision (Rouge-L): {0:.2f}\".format(precision))\nprint(\"The Recall (Rouge-L):    {0:.2f}\".format(recall))\nprint(\"The F-Measure (Rouge-L): {0:.2f}\".format(fmeasure))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}